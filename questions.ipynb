{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser, CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(textlines):\n",
    "    if isinstance(textlines, str):\n",
    "        tokens = word_tokenize(textlines)\n",
    "        pos_tagger = nltk.pos_tag(tokens)\n",
    "        ner_tags = ne_chunk(pos_tagger)\n",
    "        return ner_tags\n",
    "    else:\n",
    "        sent_tokens = []\n",
    "        for line in textlines:\n",
    "            if len(line) > 0:\n",
    "                sent_tokens = sent_tokens + sent_tokenize(line)\n",
    "\n",
    "        word_tokens = []\n",
    "        for sentence in sent_tokens:\n",
    "            word_tokens.append(word_tokenize(sentence))\n",
    "\n",
    "        pos_tagger = []\n",
    "        for sent in word_tokens:\n",
    "            pos_tagger.append(nltk.pos_tag(sent))\n",
    "\n",
    "        ner_tags = []\n",
    "        for sent in pos_tagger:\n",
    "            ner_tags.append(ne_chunk(sent))\n",
    "        return sent_tokens, word_tokens, ner_tags\n",
    "\n",
    "\n",
    "def pos_tags_combining(pos_tagger):\n",
    "    # Input: list of post tagged sentences\n",
    "    for sent in pos_tagger:\n",
    "        NNP_flag = False\n",
    "        i = 0\n",
    "        while i < len(sent):\n",
    "            if sent[i][1] == 'NNP' and NNP_flag is False:\n",
    "                NNP_flag = True\n",
    "            elif sent[i][1] == 'NNP' and NNP_flag is True:\n",
    "                word = sent[i - 1][0] + ' ' + sent[i][0]\n",
    "                sent[i - 1] = (word, 'NNP')\n",
    "                sent.remove(sent[i])\n",
    "            elif sent[i][1] != 'NNP' and NNP_flag is True:\n",
    "                NNP_flag = False\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open('a2.txt', encoding=\"utf8\")\n",
    "text = fd.read()\n",
    "textlines = text.split('\\n')\n",
    "\n",
    "sent_tokens, word_tokens, ner_tags = pre_processing(textlines)\n",
    "\n",
    "parses = []\n",
    "parser = CoreNLPParser(url='http://localhost:9000')\n",
    "# parser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_sentences = parser.raw_parse_sents(sent_tokens[:1])\n",
    "# parsed_sentences = [next(i) for i in parsed_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(parsed_sentences[0])\n",
    "# parsed_sentences[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"Ash is running away\"\n",
    "# itr = parser.raw_parse_sents([sent])\n",
    "# print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees = [next(i) for i in itr]\n",
    "# print(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"Ash is running away\"\n",
    "# tagged_sent = nltk.pos_tag(sent.split())\n",
    "# tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_quest(pos_tagged_sent):\n",
    "    noun_tags = set(['NNP', 'NNPS'])\n",
    "    verb_tags = set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD'])\n",
    "    punctuations = set(['.', ',', '(', ')', '[', ']', ';'])\n",
    "#     # Find the number of NNPs\n",
    "#     all_NNP = set()\n",
    "#     for w in pos_tagged_sent:\n",
    "#         if w[1]=='NNP':\n",
    "#             all_NNP.add(w)\n",
    "    # Find those NNP which are follwoed by verbs\n",
    "    verb_NNP = set()\n",
    "    for i,w in enumerate(pos_tagged_sent):\n",
    "        if i<len(pos_tagged_sent)-1 and w[1]=='NNP' and pos_tagged_sent[i+1][1] in verb_tags:\n",
    "            verb_NNP.add(w)\n",
    "    # Make questions for all the NNPs one by one\n",
    "#     questions = []\n",
    "#     for nnp in all_NNP:\n",
    "#         quest = []\n",
    "#         for w in pos_tagged_sent:\n",
    "#             if w==nnp:\n",
    "#                 quest += ['who']\n",
    "#             else:\n",
    "#                 quest += [w[0]]\n",
    "#         questions.append(' '.join(quest)+'?')\n",
    "    # Make question if noun followed by verb\n",
    "    questions = []\n",
    "    start = 0\n",
    "    for nnp in verb_NNP:\n",
    "        quest = []\n",
    "        for w in pos_tagged_sent:\n",
    "            if w==nnp:\n",
    "                quest += ['who']\n",
    "                start = 1\n",
    "            else:\n",
    "                if start:\n",
    "                    if w[1] in punctuations:\n",
    "                        break\n",
    "                    quest += [w[0]]\n",
    "                    \n",
    "        questions.append(' '.join(quest)+'?')\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokens[0:100]\n",
    "# sentences = [\"Arindam ran away from the Professor\"]\n",
    "# print(sentences)\n",
    "tokenized_sents = [word_tokenize(sent) for sent in sentences]\n",
    "# print(tokenized_sents)\n",
    "pos_tagged_sents = [nltk.pos_tag(sent) for sent in tokenized_sents]\n",
    "pos_tags_combining(pos_tagged_sents)\n",
    "# print(pos_tagged_sents[11])\n",
    "# print(who_quest(pos_tagged_sents[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who has appeared in all episodes of the anime?\n",
      "who has gone on to become one of the most well-known and recognizable animated characters of all-time?\n",
      "who leagues as of yet?\n",
      "who was as a child?\n",
      "who revived?\n",
      "who noted in an interview that between Japanese and American reactions to the series?\n",
      "who legend Shigeru Miyamoto?\n",
      "who noted the contrast between the characters ' relationship in the games and anime ; while in the games they were rivals?\n",
      "In an interview Tajiri noted the contrast between the characters ' relationship in the games and anime ; while in the games they were rivals?\n",
      "who replied `` No?\n",
      "When asked if either who would equal or surpass Shigeru?\n",
      "who storyline arc?\n",
      "who received a redesign in the Best Wishes?\n",
      "who received a major design overhaul for the Sun and Moon anime series?\n",
      "who has always provided the original Japanese voice of Ash?\n",
      "who provided the voice of Ash in the first eight seasons of the English adaption of the Pokémon anime?\n",
      "who stated that the character was temporarily named Casey?\n",
      "who was the only one in the booth when she recorded her lines?\n",
      "who enjoyed playing Ash because of his `` low?\n",
      "who was a little boy?\n",
      "who got separated from his group?\n",
      "who was forced to take shelter under a hollow tree?\n",
      "who kept him safe and warm?\n",
      "who had attended a Pokémon Summer Camp?\n",
      "who had hurt her knee from falling over after being startled by a Poliwag?\n",
      "who had used his handkerchief to wrap up her injury?\n",
      "who was offered a choice between three Pokémon as his starter : Bulbasaur?\n",
      "who had been taken by other trainers?\n",
      "who left Pallet Town to start his journey?\n",
      "who has traveled the world of Pokémon?\n",
      "who?\n",
      "who was able to go to the 5th round?\n",
      "who was eliminated from his first Pokémon League competition in a very unflattering manner?\n",
      "who traveled to the Orange Islands with Misty and Tracey Sketchit?\n",
      "who was able to win his battle against Drake?\n",
      "who exists only in the anime series?\n",
      "who was able to defeat his 1st new rival Morrison?\n",
      "who decides to challenge the Kanto `` Battle Frontier '' and its 7 Brains?\n",
      "who defeats all of the Brains and acquires all the 7 Battle Frontier symbols?\n",
      "who wanting to learn more about Pokémon and get even stronger?\n",
      "who wins all 8 Sinnoh gym badges?\n",
      "who has ever made it into the semi-finals in a Pokémon League?\n",
      "who is matched against Tobias?\n",
      "who is eventually defeated by Tobias?\n",
      "who concludes his participation in the Sinnoh League with an impressive new ranking?\n",
      "who continues his journey to the Unova region?\n",
      "who is then defeated by Virgil?\n"
     ]
    }
   ],
   "source": [
    "questions = [who_quest(sent) for sent in pos_tagged_sents]\n",
    "# print(questions)\n",
    "for quest in questions:\n",
    "    for q in quest:\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
